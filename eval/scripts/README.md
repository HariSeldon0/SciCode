## Update 7/5/24

 - ## **Convert the notebook input into json**
   ```
   python evaluation/scripts/notebook_to_json.py --single-step
   python evaluation/scripts/notebook_to_json.py
   ```
    - result folder: `evaluation/problem_json`
     
 - ## **Generate LLM code**
   
   To run the script, go to the root of this repo and use the following command:
   
   ```bash
   python evaluation/scripts/gencode_json.py [options]
   ```

   ### Command-Line Arguments
   - `--model` - Specifies the model name used for generating responses.
   - `--problem-background`
     - `0`: No background.
     - `1`: Includes background written by scientists.
     - `2`: Includes background based on Language Models' responses (required to generate background with `genbg.py` first).
     - `3`: Includes LM background as a comment.
   - `--llm-code` - If enabled, includes previously generated code in the prompt instead of the gold solution code.
   - `--output-dir` - Directory to store the generated code outputs (Default: `evaluation/eval_results/generated_code`).
   - `--input-dir` - Directory containing the JSON files describing the problems (Default: `evaluation/problem_json`).
   - `--prompt-dir` - Directory where prompt files are saved (Default: `evaluation/eval_results/prompt`).
   - `--llm-bg-dir` - Directory to store the background information generated by LMs (Default: `evaluation/eval_results/llm_bg_json`, required if `problem-background` is set to `2`).
   - `--temperature` - Controls the randomness of the generation (Default: 0).

  
   ### Example Usage
    - prompt `without background` and `with golden solution code`
    ```
    python evaluation/scripts/gencode_json.py --model "model_name" --problem-background 0 
    ```
   - prompt `with scientist background` and `with golden solution code`
    ```
    python evaluation/scripts/gencode_json.py --model "model_name" --problem-background 1
    ```
   - prompt `with LM background as comment` and `with golden solution code`
    ```
    python evaluation/scripts/genbg.py --model "model_name"
    python evaluation/scripts/gencode_json.py --model "model_name" --problem-background 3
    ```
    - prompt `without background` and `with previously generated code`
    ```  
    python evaluation/scripts/gencode_json.py --model "model_name" --problem-background 0 --llm-code
    ```
    - prompt `with scientist background` and `with previously generated code`
    ```  
    python evaluation/scripts/gencode_json.py --model "model_name" --problem-background 1 --llm-code
    ```
    - prompt `with LM background as comment` and `with previously generated code`
    ```
    python evaluation/scripts/genbg.py --model "model_name"
    python evaluation/scripts/gencode_json.py --model "model_name" --problem-background 3 --llm-code
    ```
    
 - ## **Evaluate generated code**

   Download `test_data.h5` at the path `evaluation/test_data.h5`.
   
   To run the script, go to the root of this repo and use the following command:
   
   ```bash
   python evaluation/scripts/test_generated_code.py
   ```
